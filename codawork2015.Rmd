---
title: "Presentation"
author: "Marc Comas-Cuf√≠"
date: "13 May 2015"
output: html_document
---

```{r, include=FALSE}
library(ggplot2)
library(mclust)
library(dplyr)
library(mixpack)
options(width=200)
knitr::opts_chunk$set(comment = " ", echo = FALSE, warning = FALSE)
rnormmix = function(n, pi= c(1/3, 1/3, 1/3), 
                    mean = c(-2, 0, 2),
                    sd = c(1, 1, 1)){
  df = apply(cbind(mean, sd), 1, function(pars) rnorm(n, mean=pars[1], sd=pars[2]))
  z = sapply(sample(1:3, size = n, replace = TRUE, prob = pi), function(i) 1:3 == i)
  df[t(z)]
}
dnormmix = function(x, pi= c(1/3, 1/3, 1/3), 
                    mean = c(-2, 0, 2),
                    sd = c(1, 1, 1)){
  df = apply(cbind(pi, mean, sd), 1, function(pars) pars[1] * dnorm(x, mean=pars[2], sd=pars[3]))
  apply(df, 1, sum)
}
cnormmix = function(x, pi= c(1/3, 1/3, 1/3), 
                    mean = c(-2, 0, 2),
                    sd = c(1, 1, 1),
                    class = 1:length(pi)){
  df = apply(cbind(pi, mean, sd), 1, function(pars) pars[1] * dnorm(x, mean=pars[2], sd=pars[3]))
  as.factor(class[apply(df, 1, which.max)])
}
get_sample = function(seed = 2){
  set.seed(seed)
  Pi = c(2/9, 4/9, 3/9)
  Mean = c(-2, 3.5, 5)
  Sd = c(0.65,1.2,0.8)
  df = data.frame('x' = rnormmix(n = 100,  pi = Pi, mean = Mean, sd = Sd))
  df$f = dnormmix(df$x, pi = Pi, mean = Mean, sd = Sd)
  df$class = cnormmix(df$x, pi = Pi, mean = Mean, sd = Sd)
  df$class2 = cnormmix(df$x, pi = Pi, mean = Mean, sd = Sd, class=c(1,2,2))
  df$f1 = Pi[1] * dnorm(df$x, mean = Mean[1], sd = Sd[1]) / df$f
  df$f2 = Pi[2] * dnorm(df$x, mean = Mean[2], sd = Sd[2]) / df$f
  df$f3 = Pi[3] * dnorm(df$x, mean = Mean[3], sd = Sd[3]) / df$f
  df.dens = data.frame('x' = seq(-5, 10, 0.2))
  df.dens$f = dnormmix(df.dens$x, pi = Pi, mean = Mean, sd = Sd)
  df.dens$f1 = Pi[1] * dnorm(df.dens$x, mean = Mean[1], sd = Sd[1])
  df.dens$f2 = Pi[2] * dnorm(df.dens$x, mean = Mean[2], sd = Sd[2])
  df.dens$f3 = Pi[3] * dnorm(df.dens$x, mean = Mean[3], sd = Sd[3])
  list('sample' = df, 'density' = df.dens)
}
```

# Presenting the goal with a toy sample

Let's assume we have the following sample $X = \{x_1,\dots,x_{100} \}$:

```{r, fig.width=4, fig.height=3}
v_xlim = c(-5, 8)
X = get_sample()
df = X$sample
df.dens = X$density

(p1 <- ggplot() + 
  geom_histogram(data=df, aes(x=x, y = ..density..), binwidth=0.5, col='black', fill='white') +
  geom_segment(data=df, aes(x=x, xend=x, y=0, yend=0.01), alpha=1) +
  theme_bw() + xlim(v_xlim) + theme(axis.ticks.y = element_blank(), axis.text.y = element_blank()) +
  ylab(NULL) + xlab('X'))
```

Just looking to the plot, _how many group do you see?_ I think, it is natural to see two group; one formed with positive values and another formed with the negative ones. 

```{r, fig.width=4, fig.height=3}
df$km = ifelse(df$x < 0, '1', '2')
ggplot() + 
  geom_histogram(data=df, aes(x=x, fill=km), binwidth=0.5, alpha=0.15, col='black') +
  geom_segment(data=df, aes(x=x, xend=x, y=0, yend=0.5, col=km), alpha=1) +
  theme_bw() + xlim(v_xlim) + theme(axis.ticks.y = element_blank(), axis.text.y = element_blank()) +
  ylab(NULL) + xlab('X')  + theme(legend.position="none")
```

## Common approach in parametric clustering

The most common approach in model based clustering starts by fitting a finite mixture of distribution, for example Gaussian distributions. One way to select the number of components is using the BIC criteria. Using this criteria, we get that the best way to represent our data is using $3$ components

\[
f(x) = \color{black}{\pi_1 f_1(x;\theta_1)} + \color{black}{\pi_2 f_2(x;\theta_2)} + \color{black}{\pi_3 f_3(x;\theta_3)}.
\]

After calculating the parameters $\{\theta_1, \dots, \theta_3$, we get the following mixture

```{r, fig.width=4, fig.height=3}
v_xlim = c(-5, 8)
(p2 <- p1 + geom_line(data=df.dens, aes(x=x, y=f), size = 1))
```

We can think a the a finite mixture as $3$ different distributions generating sample independently with $\pi_1$, $\pi_2$, $\pi_3$ being the proportions of sample expected to be generated by component $f_1$, $f_2$, $f_3$ respectively.

\[
f(x) = \color{red}{\pi_1 f_1(x;\theta_1)} + \color{green}{\pi_2 f_2(x;\theta_2)} + \color{blue}{\pi_3 f_3(x;\theta_3)} 
\]

```{r, fig.width=4, fig.height=3}
(p3 <- p1 +
  geom_line(data=df.dens, aes(x=x, y=f), size = 1) +
  geom_line(data=df.dens, aes(x=x, y=f1), size = 1, col = 'red', alpha=0.6) + 
  geom_line(data=df.dens, aes(x=x, y=f2), size = 1, col = 'green', alpha=0.6) + 
  geom_line(data=df.dens, aes(x=x, y=f3), size = 1, col = 'blue', alpha=0.6))
```

To classifiy, the observations are assigned to the Gaussian distribution, $f_j(x;\theta_j)$, they most likely belong to

```{r, fig.width=4, fig.height=3}
(p1c <- ggplot() + 
  geom_histogram(data=df, aes(x=x, y = ..density..), binwidth=0.5, col='black', fill='white', alpha=0.6) +
  geom_line(data=df.dens, aes(x=x, y=f1), size = 1, col = 'red', alpha=0.6) + 
  geom_line(data=df.dens, aes(x=x, y=f2), size = 1, col = 'green', alpha=0.6) + 
  geom_line(data=df.dens, aes(x=x, y=f3), size = 1, col = 'blue', alpha=0.6) + 
  geom_segment(data=df, aes(x=x, xend=x, y=0, yend=0.01, col=class), alpha=1) + 
  theme_bw() +  theme(legend.position="none") +xlim(v_xlim) + 
   theme(axis.ticks.y = element_blank(), axis.text.y = element_blank()) + ylab(NULL) +  
   scale_color_manual(values = c("red", "green", "blue")))
```

As we can see, the notion of clustering changes from the notion we saw in the beginning. We have considered three components and we have assigned each observation $x_i$ to the component $j$ such that posterior probability

\[
\tau_{ij} = \frac{\pi_j f_j(x_i;\theta_j)}{\sum_{\ell=1}^3 \pi_\ell f_\ell(x_i;\theta_\ell)}
\]

is maximum, in other words, to the highest component

\[
\left(\color{red}{\frac{\pi_1 f_1(x_i;\theta_1)}{\sum_{\ell=1}^3 \pi_\ell f_\ell(x_i;\theta_\ell)}}, \color{green}{\frac{\pi_2 f_2(x_i;\theta_2)}{\sum_{\ell=1}^3 \pi_\ell f_\ell(x_i;\theta_\ell)}}, \color{blue}{\frac{\pi_3 f_3(x_i;\theta_3)}{\sum_{\ell=1}^3 \pi_\ell f_\ell(x_i;\theta_\ell)}} \right)
\]

or simplifying, to the highest component

\[
\left(\color{red}{\pi_1 f_1(x_i;\theta_1)}, \color{green}{\pi_2 f_2(x_i;\theta_2)}, \color{blue}{\pi_3 f_3(x_i;\theta_3)} \right)
\]

An approach introduced by different authors consist in consisdering the mixture as a mixture of Gaussian mixtures, and classify according to this new mixture. For example, decompose mixture $f$ with the two components

\[
f(x) = \color{red}{\pi_1 f_1(x;\theta_1)} + \color{green}{ (\pi_2+\pi_3)\left\{ \frac{\pi_2}{\pi_2+\pi_3} f_2(x;\theta_2) + \frac{\pi_3}{\pi_2+\pi_3} f_3(x;\theta_3) \right\}}
\]

and classify each observation to the component

\[
\left(\color{red}{\pi_1 f_1(x_i;\theta_1)}, \color{green}{\pi_2 f_2(x_i;\theta_2)}+\color{green}{\pi_3 f_3(x_i;\theta_3)} \right)
\]

```{r, fig.width=4, fig.height=3, fig.show='hold'}
(p32 <- p1 +
  geom_line(data=df.dens, aes(x=x, y=f), size = 1) +
  geom_line(data=df.dens, aes(x=x, y=f1), size = 1, col = 'red', alpha=0.6) + 
  geom_line(data=df.dens, aes(x=x, y=f2+f3), size = 1, col = 'green', alpha=0.6))

(p12c <- ggplot() + 
  geom_histogram(data=df, aes(x=x, y = ..density..), binwidth=0.5, col='black', fill='white', alpha=0.6) +
  geom_line(data=df.dens, aes(x=x, y=f1), size = 1, col = 'red', alpha=0.6) + 
  geom_line(data=df.dens, aes(x=x, y=f2+f3), size = 1, col = 'green', alpha=0.6) + 
  geom_segment(data=df, aes(x=x, xend=x, y=0, yend=0.01, col=class), alpha=1) + 
  theme_bw() +  theme(legend.position="none") +xlim(v_xlim) + 
   theme(axis.ticks.y = element_blank(), axis.text.y = element_blank()) + ylab(NULL) +  
   scale_color_manual(values = c("red", "green", "green")))
```

# Hierarchical mixture sequence

```{r, fig.width=8.5, fig.height=3}
set.seed(3)
ms = c(runif(3, min = -2 , max= -1), runif(3, min = 0 , max= 1), runif(3, min = 2 , max= 3))
x = Reduce('c', lapply(ms, function(m) rnorm(1000, m, 0.1)))
mc = Mclust(x, G = 2:10)

df.d = data.frame(
  x = x,
  class = mc$classification)

p = ggplot() + 
  geom_histogram(data=df.d, aes(x=x, y = ..density..), binwidth=0.1, col='black', fill='white', alpha=0.6)
for(i in 1:mc$G){
  df.d.dens = data.frame(x = seq(-3, 4, length=1000)) 
  func = function(x) mc$parameters$pro[i] * dnorm(x, mean = mc$parameters$mean[i], sd = sqrt(mc$parameters$variance$sigmasq) )
  p = p + geom_line(data=df.d.dens %>% mutate(f = func(x)), aes(x=x, y=f), size = 1, col = rainbow(mc$G)[i], alpha=0.99)
}
p + theme_bw() +  theme(legend.position="none") + 
   theme(axis.ticks.y = element_blank(), axis.text.y = element_blank()) + ylab(NULL)
```

```{r}
hp = get_hierarchical_partition(mc$z, lambda = function(v_tau, a, b) if(which.max(v_tau) == b) 1 else 0, omega = function(v_tau, a) v_tau[a])
```

```{r, fig.width=8.5, fig.height=3}
p = ggplot() + 
  geom_histogram(data=df.d, aes(x=x, y = ..density..), binwidth=0.1, col='black', fill='white', alpha=0.6)

for(i in hp[[7]]){
  df.d.dens = data.frame(x = seq(-3, 4, length=1000)) 
  func = function(x) Reduce(`+`, lapply(i, function(i) mc$parameters$pro[i] * dnorm(x, mean = mc$parameters$mean[i], sd = sqrt(mc$parameters$variance$sigmasq) )))
  p = p + geom_line(data=df.d.dens %>% mutate(f = func(x)), aes(x=x, y=f), size = 1, col = rainbow(mc$G)[i[[1]]], alpha=0.99)
}

p + theme_bw() +  theme(legend.position="none") + 
   theme(axis.ticks.y = element_blank(), axis.text.y = element_blank()) + ylab(NULL)
```

```{r, fig.width=8.5, fig.height=3}
p = ggplot() + 
  geom_histogram(data=df.d, aes(x=x, y = ..density..), binwidth=0.1, col='black', fill='white', alpha=0.6)

for(i in hp[[6]]){
  df.d.dens = data.frame(x = seq(-3, 4, length=1000)) 
  func = function(x) Reduce(`+`, lapply(i, function(i) mc$parameters$pro[i] * dnorm(x, mean = mc$parameters$mean[i], sd = sqrt(mc$parameters$variance$sigmasq) )))
  p = p + geom_line(data=df.d.dens %>% mutate(f = func(x)), aes(x=x, y=f), size = 1, col = rainbow(mc$G)[i[[1]]], alpha=0.99)
}

p + theme_bw() +  theme(legend.position="none") + 
   theme(axis.ticks.y = element_blank(), axis.text.y = element_blank()) + ylab(NULL)
```

```{r, fig.width=8.5, fig.height=3}
p = ggplot() + 
  geom_histogram(data=df.d, aes(x=x, y = ..density..), binwidth=0.1, col='black', fill='white', alpha=0.6)

for(i in hp[[5]]){
  df.d.dens = data.frame(x = seq(-3, 4, length=1000)) 
  func = function(x) Reduce(`+`, lapply(i, function(i) mc$parameters$pro[i] * dnorm(x, mean = mc$parameters$mean[i], sd = sqrt(mc$parameters$variance$sigmasq) )))
  p = p + geom_line(data=df.d.dens %>% mutate(f = func(x)), aes(x=x, y=f), size = 1, col = rainbow(mc$G)[i[[1]]], alpha=0.99)
}

p + theme_bw() +  theme(legend.position="none") + 
   theme(axis.ticks.y = element_blank(), axis.text.y = element_blank()) + ylab(NULL)
```

```{r, fig.width=8.5, fig.height=3}
p = ggplot() + 
  geom_histogram(data=df.d, aes(x=x, y = ..density..), binwidth=0.1, col='black', fill='white', alpha=0.6)

for(i in hp[[4]]){
  df.d.dens = data.frame(x = seq(-3, 4, length=1000)) 
  func = function(x) Reduce(`+`, lapply(i, function(i) mc$parameters$pro[i] * dnorm(x, mean = mc$parameters$mean[i], sd = sqrt(mc$parameters$variance$sigmasq) )))
  p = p + geom_line(data=df.d.dens %>% mutate(f = func(x)), aes(x=x, y=f), size = 1, col = rainbow(mc$G)[i[[1]]], alpha=0.99)
}

p + theme_bw() +  theme(legend.position="none") + 
   theme(axis.ticks.y = element_blank(), axis.text.y = element_blank()) + ylab(NULL)
```

```{r, fig.width=8.5, fig.height=3}
p = ggplot() + 
  geom_histogram(data=df.d, aes(x=x, y = ..density..), binwidth=0.1, col='black', fill='white', alpha=0.6)

for(i in hp[[3]]){
  df.d.dens = data.frame(x = seq(-3, 4, length=1000)) 
  func = function(x) Reduce(`+`, lapply(i, function(i) mc$parameters$pro[i] * dnorm(x, mean = mc$parameters$mean[i], sd = sqrt(mc$parameters$variance$sigmasq) )))
  p = p + geom_line(data=df.d.dens %>% mutate(f = func(x)), aes(x=x, y=f), size = 1, col = rainbow(mc$G)[i[[1]]], alpha=0.99)
}

p + theme_bw() +  theme(legend.position="none") + 
   theme(axis.ticks.y = element_blank(), axis.text.y = element_blank()) + ylab(NULL)
```

# Approaches used to combine the components of a mixture

There are different approaches to merge the components of a finite mixture 

* The ridgeline unimodal method [Ray and Lindsay (2005)]
* The ridgeline ratio method [Hennig (2010)]
* The dip test method [Tantrum et al. (2003)]
* The Bhattacharyya distance method [Hennig (2010)]
* Directly estimated misclassification probabilities (DEMP) method [Hennig (2010)]
* The prediction strength method [Tibshirani and Walther (2005)]
* Entropy minimization method [Baudry et el. (2010)]

From all of them, here we are interested on those methods depending only on the posterior probabilities

\[
\tau_{ij} = \frac{\pi_j f_j(x_i;\theta_j)}{\sum_{\ell=1}^3 \pi_\ell f_\ell(x_i;\theta_\ell)}.
\]

In fact, we are interested on methods based on the posterior probability vectors

\[
\left( \tau_{i1}, \tau_{i2}, \dots, \tau_{ik} \right)
\]

From the previous methods, only the DEMP method and the Entropy minimization method are based on the posterior probability vectors. Next we revise this two methods and discuss the notion of confusion between component they are considering.


## The entropy criteria

Baudry et al proposed to combine those two components for which after merging the components the total Shannon Entropy 

\[
- \sum_{i=1}^n \sum_{j=1}^k \tau_ij log(\tau_ij)
\]

is minimized. The Shannon Entropy

```{r, fig.width=4, fig.height=3}
xlog = function(x) ifelse(x == 0, 0, x * log(x))
entr = function(x) -(xlog(x) + xlog(1-x))

ggplot() + geom_line(data = data.frame(x = seq(0, 1, 0.001)) %>% mutate(ent = entr(x)), aes(x = x, y = ent)) + theme_bw() + 
  xlab(expression(tau[2])) +
  ylab('Shannon entropy')
```


```{r, fig.width=4, fig.height=3, fig.show='hold'}
p = ggplot()
for(i  in 1:5){
  other = seq(0.2, 0.8, length.out=5)[i]
  p = p + geom_line(data = data.frame(x = seq(0, 1-other, 0.0001)) %>% mutate(ent_diff = xlog(1-other) - xlog(x) - xlog(1-other-x)), aes(x = x, y = ent_diff), col=i)
}
p + theme_bw() + 
  xlab(expression(tau[2])) + ylab('Shannon entropy') + coord_cartesian(xlim=c(0,1))

p = ggplot()
for(i  in 1:5){
  other = seq(0.2, 0.8, length.out=5)[i]
  p = p + geom_line(data = data.frame(x = seq(0, 1-other, 0.0001)) %>% mutate(ent_diff = -log((1-other-x)/x)^2), aes(x = x, y = ent_diff), col=i)
}
p + theme_bw() + 
  xlab(expression(tau[2])) + ylab('Aitchison distance') + coord_cartesian(xlim=c(0,1), ylim=c(-5, 1))
```


### The confusion measure

\[
\sum_{i=1}^n  (\hat{\tau}_{i I_a}+\hat{\tau}_{i I_b}) \log(\hat{\tau}_{iI_a} + \hat{\tau}_{i I_b}) - 
\sum_{i=1}^n \left\{ \hat{\tau}_{i I_a} \log(\hat{\tau}_{i I_a}) + \hat{\tau}_{iI_b} \log(\hat{\tau}_{i I_b})\right\}
\]

## The missclassification criteria

Hennig proposed to merge those two components for which thr probability of assigning one observation to the other is maximum.


Conditioning that $x_i$ was generated by component 1, the higher the proportion of $\tau_2$ with respect $\tau_1$ higher the confusion between component 1 and 2.

```{r, fig.width=4.5, fig.height=7, fig.show='hold'}
d <- lapply(c(0.1, 0.2, 0.3, 0.4, 0.5), function(other) data.frame(x = seq(0, 1-other, 0.0001)) %>% 
                                                            mutate(demp = ifelse(1-other-x > x & other < 1-other-x, 1, 0),
                                                                   other = as.character(other))) %>% bind_rows
ggplot() + geom_line(data = d, aes(x = x, y = demp, col=other)) + theme_bw() + 
  xlab(expression(tau[2])) + ylab('DEMP criteria') + coord_cartesian(xlim=c(0,1)) + facet_grid(other~.)

d <- lapply(c(0.1, 0.2, 0.3, 0.4, 0.5), function(other) data.frame(x = seq(0, 1-other, 0.0001)) %>% 
                                                            mutate(log = log((1-other-x) / x),
                                                                   other = as.character(other))) %>% bind_rows
ggplot() + geom_line(data = d, aes(x = x, y = log, col=other)) + theme_bw() + 
  xlab(expression(tau[2])) + ylab('Log-ratio criteria') + coord_cartesian(xlim=c(0,1)) + facet_grid(other~.)
```

```{r, fig.width=4, fig.height=3, eval=FALSE}
other = 0
ggplot() + geom_line(data = data.frame(x = seq(0.01, 1-other-0.01, 0.01)) %>% mutate(log = log((1-other-x) / x)), aes(x = x, y = log)) + theme_bw() + 
  xlab(expression(tau[2])) + ylab('Logratio') + coord_cartesian(xlim=c(0,1))
```

### The confusion measure

\[
P(\{x_i \text{ assigned to } I_b\}|\{x_i \text{ generated by } I_a\}) = \frac{P( \{x_i \text{ assigned to } I_b\}, \{x_i \text{ generated by } I_a\}) }{ P(\{x_i \text{ generated by } I_a\}) }
\]

\[
\frac{ \frac{1}{n} \sum_{i=1}^n {\hat{\tau}_{i I_a} \mathbb{1}\left( \forall j\; \hat{\tau}_{i I_{b}} \geq \hat{\tau}_{i I_j} \right)}}{ \pi_{I_a} }
\]

\[
\frac{ \sum_{i=1}^n {\hat{\tau}_{i I_a} \mathbb{1}\left( \forall j\; \hat{\tau}_{i I_{b}} \geq \hat{\tau}_{i I_j} \right)}}{\sum_{i=1}^n \hat{\tau}_{i I_a} }
\]

# Introducing a CoDa approach to previous measures

# A generic formulation

\[
\frac{\sum_{i=1}^n \omega(\hat{\tau}_{i \mathcal{P}_s}, I_a) \lambda(\hat{\tau}_{i \mathcal{P}_s}, I_a, I_b)}{\sum_{i=1}^n \omega(\hat{\tau}_{i \mathcal{P}_s}, I_a) }
\]

# Calaix de sastre: extres

```{r, include=FALSE}
library(ggtern)
library(compositions)
```

```{r, fig.width=4, fig.height=4.5, fig.show='hold'}
ggtern() + geom_Tline(
  data=data.frame(levels = c(0.1, 0.2, 0.3, 0.4, 0.5), 
                  other = as.factor(c(0.1, 0.2, 0.3, 0.4, 0.5))), 
  aes(Tintercept=levels, col=other), size=1.5) + 
  Tlab('other') + Llab(expression(tau[1])) + Rlab(expression(tau[2])) + 
  theme_bw()
```

```{r, fig.width=4, fig.height=4.5, fig.show='hold'}
plot(df %>% select(f1, f2, f3) %>% acomp, col=df$class)
plot(scale(df %>% select(f1,f2,f3) %>% acomp) %>% data.frame %>% acomp, col=df$class)
# ggtern( data = df %>% select(f1, f2, f3, class) ) + geom_point(aes(x=f1, y=f2, z=f3, col=class)) + theme_bw() + scale_colour_discrete(guide=FALSE)
# 
# df.cent = scale(df %>% select(f1,f2,f3) %>% acomp) %>% data.frame
# df.cent$class = df$class
# ggtern( data = df.cent %>% select(f1, f2, f3, class) ) + geom_point(aes(x=f1, y=f2, z=f3, col=class)) + theme_bw() + scale_colour_discrete(guide=FALSE)
```

